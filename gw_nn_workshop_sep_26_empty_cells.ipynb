{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AN INTRODUCTION TO NEURAL NETWORKS**\n",
    "\n",
    "*Patrick Donnelly, Groupware Technology*\n",
    "\n",
    "This lab introduces the basic operations for training neural networks. We examine matrix multiplications and additions, convolutions, and softmax, cross-entropy, and activation functions in pure Python, NumPy, and PyTorch. By the end of this lab, you will be able to code a neural network for image classification in PyTorch and understand the mathematics behind deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the simplest network as consisting of an **input node** and an **output node**.\n",
    "\n",
    "We can then apply a **weight** to the input to generate the output. The weight multiplies the input `x` by a constant `w` to generate the output. If `w=1`, the output is simply the input.\n",
    "\n",
    "Let's build a Python function `output` that returns the product of an input `x` and a weight `w`. For the first set of cells, we'll do this together as a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does `output` return when `x=1` and `w=1`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add a **bias**, or constant value `b` added to the input\n",
    "\n",
    "By convention, we represent biases as nodes\n",
    "\n",
    "If `b=1`, we add one to our weighted input to generate our output\n",
    "\n",
    "Let's build another Python function `output_with_bias` that takes our input `x`, a weight `w`, and a bias `b` and computes the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does `output` return when `x=1` and `b=1`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now defined a line. However, when training a neural network, we don't know the values of `w` or `b`. We need to use our inputs, outputs, and a **learning algorithm** to learn `w` and `b`.\n",
    "\n",
    "Let's say we have one input/output pair, and our output is equal to $2 * input + 1$. What is our output when our input is $0$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when training a model, we don't \"know\" `w` or `b`, and there are an infinite number of pairs of $(w, b)$ that satisfy $f(0) = 1$. Make sense?\n",
    "\n",
    "With two input/output pairs, we can solve for our weight and bias. Let's write a function `linear_solver` that takes two inputs and two outputs and returns a weight and bias that fits both points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's our learned weight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this isn't how neural networks work. Neural networks (NNs) don't solve linear models analytically. NNs use something called **backpropagation**, modifying the value of the weights and biases to minimize the discrepancy between actual and predicted outputs. This is super inefficient in our example, but don't worry about that for now.\n",
    "\n",
    "Let's go back to our example with $(x,y) = (0,1)$. Our **loss function**, $L(y, Y)$ tells us how well our network is performing. The loss function measures the discrepancy between actual loss $y$ and predicted loss $Y$.\n",
    "\n",
    "Let's start with an obvious loss function: absolute difference, where $L(y, Y) = |y - Y|$. What does `absolute_difference` look like in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need initial values of `w` and `b` to generate predicted outputs. We'll start with zero initialization (which is generally a terrible idea, but that's okay for now). What's our output (with bias) when $(w, b) = (0, 0)$? Let's call our output `Y` and then print the value of `Y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our input $0$ will generate output $0$, and thus our loss is $|1-0| = |1| = 1$. However, the \"true\" value of $y$ is generated (deterministically) from $y = 2x + 1$. What's the \"true\" value of our output when our input is $0$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute our loss from our actual output `y` and predicted output `Y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we calculate our loss, we need to update our weight and bias. We update our \"network\" by taking the derivatives of the loss function with respect to our weight and bias. We call these derivatives the **gradients** of our error function with respect to our weight and bias.\n",
    "\n",
    "However, there's a problem with our loss function $L(x)$: it's not differentiable when $|y - Y| = 0$. Think about the absolute value function: it's $f(x) = -x$ when $x < 0$, $f(x) = x$ when $x > 0$, and $f(x) = 0$ when $x = 0$.\n",
    "\n",
    "So for $f(x) = |x|$, $f'(x) = -1$ when $x < 0$ and $f'(x) = 1$ when $x > 0$. But at $f'(x) = 0$ we can't really tell if the function is going up or down; it's kind of a \"hard inflection point\" (is that a mathematically precise term?)\n",
    "\n",
    "We need a loss function that is continuously differentiable. Why not just square $|y - Y|$?. Now $L(y, Y) = (y - Y)^2$. What does this look like in code? Call the function `squared_difference`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What value does `squared_difference` return when `y=1` and `Y=0`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's differentiate $L(y, Y) = (y - Y)^2$ with respect to our weight $w$\n",
    "\n",
    "For $w$,\n",
    "\n",
    "$\\partial L(x)/\\partial w = \\partial/\\partial w (y - Y)^2 =$\n",
    "\n",
    "$\\partial/\\partial w (y - (wx + b))^2 =$\n",
    "\n",
    "$\\partial/\\partial w (y - wx - b)^2 =$\n",
    "\n",
    "$2(y - wx - b)(-x) =$\n",
    "\n",
    "$-2x(y - wx - b)$\n",
    "\n",
    "We can use this to build a function `dL_dW` to compute $\\partial L(x)/\\partial w$. Since $L(y, Y)$ is a function of our actual (ground truth) output $y$ and model (prediction) output $Y$, and $Y = wx + b$, we can use the input to our loss function $L(y, Y) = L(y, (x, (w, x, b))) = L(x, y, w, b)$ to update our weight (and subsequently bias). How do we code `dL_dW`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example with $(x, y) = (0, 1)$, and zero initialization: $(w, b) = (0, 0)$, compute $\\partial L(y, Y)/ \\partial w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also differentiate $L(x) = (y - Y)^2$ with respect to our bias $b$\n",
    "\n",
    "For $b$,\n",
    "\n",
    "$\\partial L(x)/\\partial b = \\partial/\\partial$b $(y - Y)^2 =$\n",
    "\n",
    "$\\partial/\\partial b (y - (wx + b))^2 =$\n",
    "\n",
    "$\\partial/\\partial b (y - wx - b)^2 =$\n",
    "\n",
    "$2(y - wx - b)(-1) =$\n",
    "\n",
    "$-2(y - wx - b)$\n",
    "\n",
    "Just as we did with our weight $w$, we can use this to build a function `dL_dB` to compute $\\partial L(y, Y)/\\partial b$. How do we code this function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same example with $(x, y) = (0, 1)$, and zero initialization: $(w, b) = (0, 0)$, compute $\\partial L(y, Y)/\\partial b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, let's now use these gradients to update our weights and biases. We'll use something called **gradient descent**:\n",
    "\n",
    "$w_{t+1} = w_{t} - \\alpha (\\partial L(y, Y)/\\partial w_{t})$\n",
    "\n",
    "Here we are updating our weight from $w_{t}$ to $w_{t+1}$ by subtracting the scaled gradient of the loss function with respect to the weight. We scale the gradient by something called the **learning rate**, conventionally represented by $\\alpha$\n",
    "\n",
    "In the next block of code, we'll augment our `dL_dw` function to update the weight from $w_{t}$ to $w_{t+1}$. Let's call this function `gd_weight`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same example $(x, y) = (0, 1)$ with zero initialization $(w, b) = (0, 0)$, and learning rate $a = 0.1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our weights don't actually update when $x = 0$ and $L(y, Y)$ is the squared error function $(y - Y)^2$. We'll need another example to update the weights. However, we can use this example to update the bias $b_{t}$ to $b_{t+1}$:\n",
    "\n",
    "$b_{t+1}$ = $b_{t}$ - $\\alpha$ ($\\partial$E(y, Y)/$\\partial$$b_{t}$)\n",
    "\n",
    "How about coding this in Python? We'll call it `gd_bias`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the output of `gd_bias` when $(x, y) = (0, 1)$, assuming zero initialization of weights and biases and a learning rate $a = 0.1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now feed a second datum generated from the line $y = 2x + 1$. When $x = 1$, $y = 2(1) + 1 = 3$. We use this to compute our new output $Y$. Let's set our predicted output `Y` equal to this output and print the output. Recall that our function is called `output_with_bias`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss for this example is the squared difference between $y=3$ and $Y=0.2$. Let's call `squared_difference` using these two arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might look worse than when $y=1$ and $Y=0$. However, consider if we had fed the $(1, 3)$ example first. What would be our loss in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our loss is decreasing, and if we update our weight and bias again, we'll see our loss continue to decrease. Let's first update our weight and print the value of our new weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's update our bias (using the weight before our update):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one more \"forward pass\" with $(x, y) = (0, 1)$ and the weights and biases updated to $0.56$ and $0$. We'll \"train\" the rest of this network by clicking through the code (rather than building the code together):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = output_with_bias (x=0, w=0.56, b=0.76)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the loss continue to decrease, not only compared to the loss computed for $(1, 3)$, but also compared to the prior weights and biases applied to $(0, 1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(squared_difference(y=1, Y=output_with_bias (x=0, w=0, b=0)))\n",
    "print(squared_difference(y=1, Y=output_with_bias (x=0, w=0, b=0.2)))\n",
    "print(squared_difference(y=1, Y=output_with_bias (x=0, w=0.56, b=0.76)))             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one final weight and bias update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gd_weight (x=0, y=1, w=0.56, b=0.76, a=0.1))\n",
    "print(gd_bias (x=0, y=1, w=0.56, b=0.76, a=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view our updated output for $(1, 3)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = output_with_bias (x=1, w=0.56, b=0.81)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see the loss decrease for $(1, 3)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(squared_difference(y=3, Y=output_with_bias (x=0, w=0, b=0)))\n",
    "print(squared_difference(y=3, Y=output_with_bias (x=0, w=0, b=0.2)))\n",
    "print(squared_difference(y=3, Y=output_with_bias (x=0, w=0.56, b=0.76)))\n",
    "print(squared_difference(y=3, Y=output_with_bias (x=0, w=0.56, b=0.81)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And similarly if we pass $(0, 1)$ through without a subsequent update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(squared_difference(y=1, Y=output_with_bias (x=0, w=0, b=0)))\n",
    "print(squared_difference(y=1, Y=output_with_bias (x=0, w=0, b=0.2)))\n",
    "print(squared_difference(y=1, Y=output_with_bias (x=0, w=0.56, b=0.76)))\n",
    "print(squared_difference(y=1, Y=output_with_bias (x=0, w=0.56, b=0.81)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by now, you're probably wondering, \"why in the world would we use a neural network for linear regression with one weight and bias and two examples?\" Good question.\n",
    "\n",
    "Let's try a less artificial (but still very artificial) example. Why are we keeping this artificial? Well, if it got any more \"real,\" it'd be hard to examine the mechanisms at work in training the network. We'd be dealing with two many parameters. Am I being too abstract already?\n",
    "\n",
    "Consider image classification. Neural networks are really good at this. However, if we started with 256 x 256 x 3 images, we'd have too many parameters to visualize what's going on. Similarly, if we let each pixel take values between 0 and 255, it'll be harder to wrap our heads around the math.\n",
    "\n",
    "So we'll start with another artificial example. Consider a 5x5 \"image.\" This \"image\" can take pixel values of either 0 or 1. Think of it as a \"true\" black-and-white image (no grayscale). We can then use this 5x5 image to represent \"images\" of digits (0 to 9). What would such an image look like? Picture an old-school scoreboard. How about a \"0\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_zero = [[0,1,1,1,0],[1,0,0,0,1],[1,0,0,0,1],[1,0,0,0,1],[0,1,1,1,0]]\n",
    "a_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or maybe a \"4\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_four = [[1,0,1,0,0],[1,0,1,0,0],[1,1,1,1,1],[0,0,1,0,0],[0,0,1,0,0]]\n",
    "a_four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about another \"4\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_four = [[0,0,1,0,0],[0,1,1,0,0],[1,1,1,1,1],[0,0,1,0,0],[0,0,1,0,0]]\n",
    "another_four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can envision a neural network that could tell us \"that's a four,\" or \"that one's an eight.\" We call this task **image classification**. But before we go any further, we're going to import our first library. I know, I like doing everything in pure Python, and I'm sure someone reading this is like, \"whatever noob, I backprop in assembly.\" Well whatever right back at you, we're using NumPy. It supports all that matrix stuff that we're gonna need. It's got this thing called a \"NumPy array\" that's just better for numerical computation, especially with vectors and matrices.\n",
    "\n",
    "By convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reencode our \"zero\" in NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_zero_array = np.array([[0,1,1,1,0],[1,0,0,0,1],[1,0,0,0,1],[1,0,0,0,1],[0,1,1,1,0]])\n",
    "a_zero_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can just pass our `a_zero` list of lists to `np.array`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "also_a_zero_array = np.array(a_zero)\n",
    "also_a_zero_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a **label** for our array of zeroes, something that will tell us that our array of zeroes is a 0 and not a 1 or a 2. This is analogous to the \"true value\" of $y$ in our simple regression example.\n",
    "\n",
    "For classification, we typically use a **one-hot encoding** for each observation. A one-hot encoding is a vector equal in length to the number of classes. In the case of digits, we have 10 classes from 0 to 9. We index the one-hot encoding by these classes, with the \"true\" class equal to 1 and the others equal to zero. So for a 0, our encoding is (in pure Python and NumPy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_zero_encoding = [1,0,0,0,0,0,0,0,0,0]\n",
    "numpy_zero_encoding = np.array(python_zero_encoding)\n",
    "print(python_zero_encoding)\n",
    "print(numpy_zero_encoding)\n",
    "numpy_zero_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stick with NumPy for here on! (Until we get to PyTorch...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_four_encoding = np.array([0,0,0,0,1,0,0,0,0,0])\n",
    "numpy_four_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's return to our simple regression example. We can reconceptualize this as a (ridiculously simple) 1x1 matrix mutiplication and addition. Let's take the example of $(0,1)$ with zero-initialized weights and biases. `x` is our 1x1 input, `y` is our 1x1 output, `W` is our 1x1 zero-initialized weight \"matrix,\" and `b` is our bias \"vector\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0])\n",
    "y = np.array([1])\n",
    "W = np.array([0])\n",
    "b = np.array([0])\n",
    "x, y, W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute our model output, we do matrix multiplication in NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.matmul(W, x) + b\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_loss = (y - Y)**2\n",
    "se_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update our weights too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_weight_mat (x, y, W, b, a):\n",
    "    W = W - a * (-2*x*(y - np.matmul(W, x) - b))\n",
    "    return \"W={}\".format(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the \"new\" weight when `a=0.1`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gd_weight (x, y, W, b, a=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's update our bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_bias_mat (x, y, W, b, a):\n",
    "    b = b - a * (-2*(y - W*x - b))\n",
    "    return \"b={}\".format(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the new bias when `b=0.1`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gd_bias (x, y, W, b, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good stuff! Now what happens if we apply the same logic to our 5x5 \"image\" and 10x1 10-class output? Well, we already got our image and output. Now we need to initialize weight and bias matrices. What size do we need? This might seem confusing at first - don't we have a dimentional mismatch between our 5x5 input and 10x1 output? We need to multiply an $n x m$ matrix by an $m x p$ matrix to get an $n x p$ matrix, and we can't even do this with the transpose of our 5 x 5 input...\n",
    "\n",
    "Ok, the solution is actually simple. We flatten our input matrix into a vector of length 5 x 5 = 25. In NumPy, this is as straightforward as removing the brackets that defined our array as a two-dimensional matrix. Let's use the `shape` attribute to examine our initial array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_zero_array = np.array([[0,1,1,1,0],[1,0,0,0,1],[1,0,0,0,1],[1,0,0,0,1],[0,1,1,1,0]])\n",
    "print(a_zero_array.shape)\n",
    "print(len(a_zero_array.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's change the shape to a one-dimensional array (vector):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_zero_vector = np.array([0,1,1,1,0,1,0,0,0,1,1,0,0,0,1,1,0,0,0,1,0,1,1,1,0])\n",
    "print(a_zero_vector.shape)\n",
    "print(len(a_zero_vector.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other thing. We actually want the shape of our vector to be (25,1), not (25,). So we can reshape the vector using `reshape`. We could have done this to our 5x5 matrix, but whatever. Also, we're gonna call it `x` since that's what we've been calling our input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = a_zero_vector.reshape(25, 1)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our weights matrix and bias vector. We need something to multiply our (25, 1) vector by to get something of shape (10, 1). Sounds like we need a (10, 25) matrix! Also, we'll have to add a vector to that for bias, and it'll need to be the same shape, so we also need a (10, 1) vector. We'll start with zero initialization, which is stupid but we'll get to that later. `zeros` is a nice built-in function in NumPy that does the trick for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.zeros((10, 25))\n",
    "b = np.zeros((10, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to make sure that `zeros` did its job for our weight matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our bias vector should also be all zeroes (zeros?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now we can compute the output of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.matmul(W, x) + b\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one more thing we need to do before computing our loss. We want to convert our model output into a probability distribution to compare it with our ground truth. For this, we use what's called a **softmax** function. The softmax function first exponentiates each of the outputs (they're outputs of the matrix multiplication and addition, but inputs to the softmax function). Then the function divides by the sum of all the exponentiated inputs. In formal terms, the softmax $S(Y)$ applied to the output vector $Y$ is defined as follows:\n",
    "\n",
    "$S(Y)_{i}$ = $\\frac {e^Y_{i}}{\\sum_{j=1}^K e^Y_{j}}$ for $i = 1, ..., K$ and $Y = (Y_{i}, ... , Y_{k})$\n",
    "\n",
    "That might look scary but it's super simple. We use the `exp` function for $e^{Y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax (output):\n",
    "    return np.exp(output)/np.sum(np.exp(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute our loss. We'll apply this loss to the output from our softmax function $Y$ and our ground truth $y$. We'll also need to reshape $y$ so that it's the same shape as $Y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = softmax(output)\n",
    "y = numpy_zero_encoding.reshape(10,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can anyone guess what `Y` looks like now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about `y`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function is something called **cross-entropy loss**. The cross-entropy function is just the negative natural logarithm of the model output for the \"ground truth\" class:\n",
    "\n",
    "$L(y, Y) = - {\\sum_{i=1}^K y_{i}log(Y_{i})}$\n",
    "\n",
    "In other words, we mutiply the ground truth by the natural log (`np.log`) of the model output for each class, sum the results, and flip the sign. Of course, for a one-hot encoding, everything will go to zero except for the ground truth class, so we're just left with the negative natural log of the model output corresponding to the ground truth class. Let's code up a function `cross_entropy` that takes an actual and predicted output and returns the cross-entropy loss. Let's do this one together again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the cross-entropy loss for our current values of `Y` and `y`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now for some more backprop. We need to take the derivative of our (cross-entropy) loss function with respect to the weights and biases. This will require us to apply the chain rule. If you don't remember that from calculus, or haven't taken calculus, don't worry. It's simple. Let $L = L(S)$ be our (cross-entropy) loss (error) function, which takes the output from our softmax function as an input.\n",
    "\n",
    "Let $S = S(y, Y)$ be our softmax function. $Y = Y(x)$ is the output of our linear transformation $Wx + b$ (for matrix $W$ and vectors $x$ and $b$) or $w_{0}x + b_{0}$ (for a particular weight $w_{0}$ and bias $b_{0}$). For (relative) simplicity, let's differentiate with respect to a particular weight $w_{0}$ and bias $b_{0}$:  \n",
    "\n",
    "$\\partial L/\\partial w_{0} = \\partial L/\\partial S * \\partial S/\\partial Y * \\partial Y/\\partial w_{0}$\n",
    "\n",
    "and\n",
    "\n",
    "$\\partial L/\\partial b_{0} = \\partial L/\\partial S * \\partial S/\\partial Y * \\partial Y/\\partial b_{0}$\n",
    "\n",
    "We'll actually start by computing the derivative of the softmax function $S(y, Y)$ with respect to the output of the linear transformation $Y = w_{0}x + b_{0}$\n",
    "\n",
    "We'll use the **quotient rule** to compute the derivative:\n",
    "\n",
    "For $f(x)$ = $\\frac{g(x)}{h(x)}$, $f'(x)$ = $\\frac{g'(x)h'(x) - h'(x)g(x)}{h(x)^2}$\n",
    "\n",
    "Recall the softmax function is $S(Y)_{i}$ = $\\frac {e^Y_{i}}{\\sum_{j=1}^K e^Y_{j}}$\n",
    "\n",
    "Let's simply things a bit further. Let's only backpropagate one of the weights: the weight connecting the zero-index input with the zero-index output. Recall the input, output, and ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[0][0])\n",
    "print(Y[0][0])\n",
    "print(y[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us? We take an input 0 (along with 24 other inputs), pass it through a linear function, apply a softmax transformation, and get 0.1 as our model output. The true output is 1. So what is the softmax function as applied to this specific weight?\n",
    "\n",
    "When taking the derivative for the same input and output node (the zero index), as we are doing here, we know that $\\partial$$e^{a(x)}$/$\\partial$$a(x)$ is $e^{a(x)}$. This is just the application of the world's simplest derivative: the derivative of $e^{x}$ is itself.\n",
    "\n",
    "Now we can apply the quotient rule from above. We'll swap in $Y$ for $x$ since our softmax vector takes the \"output\" $Y$ as its input:\n",
    "\n",
    "$g(Y) = e^{a_{i}(Y)}$\n",
    "\n",
    "$h(Y) = \\sum_{k=1}^N e^{a_{k}(Y)}$\n",
    "\n",
    "$g'(Y) = e^{a_{i}(Y)}$\n",
    "\n",
    "$h'(Y) = e^{a_{j}(Y)}$ since $e^{a_{k}(Y)} = e^{a_{j}(Y)}$ when $i = j$, and $e^{a_{k}(Y)} = 0$ when $i \\ne j$. Remember, we're differentiating the softmax function for $i$, $S_{i}$ with respect to our output $a(Y)$ for $j$, $a_{j}(Y)$\n",
    "\n",
    "So $f'(Y) = \\frac{ e^{a_{i}(Y)} \\sum_{j=1}^K e^{a_{j}(Y)} - e^{a_{j}(Y)} e^{a_{i}(Y)} } { (\\sum_{k=1}^N e^{a_{k}(Y)})^2 }$\n",
    "\n",
    "Factoring out an $e^{a_{i}(Y)}$ from the numerator:\n",
    "\n",
    "$f'(Y) = \\frac{ e^{a_{i}(Y)} (\\sum_{j=1}^K e^{a_{j}(Y)} - e^{a_{j}(Y)}) } { (\\sum_{j=1}^K e^{a_{k}(Y)})^2 }$\n",
    "\n",
    "Splitting the expression:\n",
    "\n",
    "$f'(Y) = \\frac{ e^{a_{j}(Y)} } { \\sum_{j=1}^K e^{a_{k}(Y)} } Y \\frac{ \\sum_{k=1}^N e^{a_{k}(Y)} - e^{a_{j}(Y)} } { \\sum_{k=1}^N e^{a_{k}(Y)} }$\n",
    "\n",
    "... which is magically equivalent to $S_{i}(1 - S_{j})$ (again, for our case where $i=j$)\n",
    "\n",
    "Now let's return to our cross-entropy loss:\n",
    "\n",
    "$L = - \\sum_{i=1}^N y_{i} ln(S_{i})$, where $y$ is our \"ground truth\" vector, and thus $y_{i}$ is index $i$ of the \"ground truth\" vector. Again, $S_{i}$ is the softmax function applied to index $i$ of the \"model\" or output vector.\n",
    "\n",
    "If we differentiate our cross-entropy loss with respect to our output for index $i$, we get:\n",
    "\n",
    "$\\partial L/ \\partial Y_{i} = - \\sum_{k=1}^N y_{k} \\frac{ \\partial{ln(S_{k})} } { \\partial{ Y_{i} }}$\n",
    "\n",
    "Applying the chain rule:\n",
    "\n",
    "$\\partial L/ \\partial Y_{i} = - \\sum_{k=1}^N y_{k} \\frac{ \\partial{ln(S_{k})} } { \\partial{ S_{k} }} x \\frac{ \\partial{S_{k}} }{ \\partial{Y_{i}} }$\n",
    "\n",
    "Since $\\frac {\\partial ln(x) }{\\partial{x}} = \\frac{1}{x}$,\n",
    "\n",
    "$\\partial L/ \\partial Y_{i} = - \\sum_{k=1}^N y_{k} \\frac{1}{S_{k}} x \\frac{ \\partial{S_{k}} }{ \\partial{Y_{i}} }$\n",
    "\n",
    "Now let's plug in our softmax derivative. We can decompose the expression into instances where $k=i$ and $k \\ne i$:\n",
    "\n",
    "$\\partial L/ \\partial Y_{i} = - y_{i}(1 - S_{i}) - \\sum_{k \\ne i}^N y_{k} \\frac{1}{S_{k}}(-S_{k} S_{i})$\n",
    "\n",
    "$= - y_{i}(1 - S_{i}) + \\sum_{k \\ne i}^N y_{k} S_{i}$\n",
    "\n",
    "$= - y_{i} + y_{i} S_{i} + \\sum_{k \\ne i}^N y_{k} S_{i}$\n",
    "\n",
    "$= S_{i} (y_{i} + \\sum_{k \\ne i}^N y_{k}) - y_{i}$\n",
    "\n",
    "Since $y$ sums up to 1 (it's one 1 and a bunch of zeroes - a \"one-hot\" encoding), $\\sum_{k = 1}^N y_{k} = 1$ and $y_{i} + \\sum_{k \\ne 1}^N y_{k} = 1$\n",
    "\n",
    "Thus, $\\frac {\\partial L}{\\partial Y_{i}} = S_{i} - y_{i}$\n",
    "\n",
    "Yikes, that's a lot of math. The code is ridiculously simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cross_entropy(Y, y):\n",
    "    return softmax(Y) - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call `grad_cross_entropy` on our current `Y` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cross_entropy(Y, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to take a step further and update our weights $w_{i}$. Applying the chain rule:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_{i}} = \\frac{\\partial L}{\\partial Y_{i}} x \\frac{\\partial Y_{i}}{\\partial w_{i}}$\n",
    "\n",
    "$= (S_{i} - y_{i}) x_{i}$\n",
    "\n",
    "We can now update our weights!\n",
    "\n",
    "$w_{i(t+1)} = w_{it} - \\alpha \\frac{\\partial Y}{\\partial w_{it}}$\n",
    "\n",
    "$= w_{it} - \\alpha \\frac{\\partial Y}{\\partial Y_{i}} \\frac{\\partial Y_{i}}{\\partial w_{i}}$\n",
    "\n",
    "Let's first update the single weight $w_{0}$. Can we write a function that does this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_cross_entropy (Y, y, x, w, a):\n",
    "    return w - a*(softmax(Y[0]) - y[0])*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $x=0$. We'll keep our learning rate $\\alpha=0.1$. What does this function return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_cross_entropy(Y=Y, y=y, x=0, w=0, a=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh, looks like our weights aren't going to update again with $x = 0$. \n",
    "\n",
    "We could try with a different observation or different weight, but at this point we should note that zero initialization isn't ideal. Once we start adding activation functions, zero initialization will prevent our neural network from updating weights during backpropagation. More on this once we get to activation functions.\n",
    "\n",
    "Let's try something better: randomly initialized weights (and biases)! Recall that we have a 10x25 weight matrix `W` and a 10x1 bias vector `b`. We're going to replace those zero initializations with weights randomly sampled from a standard normal distribution (mean 0, variance 1). There's a NumPy function that does this called `numpy.random.randn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(10, 25)\n",
    "b = np.random.randn(10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure our weight matrix passes the eyeball test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our bias vector is more pleasing to the eyeballs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute our outputs again with these randomly initialized weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.matmul(W, x) + b\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pass that output through a softmax function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = softmax(Y)\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and compute cross entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy(S, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now backprop is going to get trickier. We have to update a bunch of weights and biases, and pretty soon we're going to be stacking layers. Hopefully by now you have a good idea of what we're doing -- it's just the partial derivative of the loss function with respect to the weights and biases, and we solve for this using the chain rule. Maybe in a future course, we'll do some more backprop in NumPy, but I think this is sufficient for an introduction to neural networks. We got a lot more to cover.\n",
    "\n",
    "Thankfully, we have something called **PyTorch** that makes things a lot easier. PyTorch is a **deep learning framework**, or collection of operations for training neural networks. Also, it looks a lot like NumPy. You'll have to have it [installed](https://pytorch.org/get-started/locally/) already. Let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as the basic unit of NumPy is the numpy array, PyTorch has a class called `Tensor`. We'll be performing our operations on `Tensor`s. PyTorch makes it easy to convert NumPy arrays to Tensors using `torch.tensor`, so we're gonna do that for our `W` matrix, and `b`, `x`, and `y` vectors.\n",
    "\n",
    "Before we get to this, we'll also change our encoding of the **target** (ground truth) from a one-hot vector to class indices. We simply do this by passing the label value, in this case `0` to `torch.tensor` and setting the output to `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that our label is in fact a `torch.tensor`. One simple way to do this is to type `y` in the interactive cell and execute the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also call `type` on `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is different than calling `type` as a `Tensor` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we called `type` without the parentheses, we'd get something different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not to mention we could also call the `dtype` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we're having too much fun. While we're at it, what's the `type` of our bias vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert `W`, `b`, `x`, and `y` from `numpy.ndarray`s to `torch.Tensor`s. We're also going to pass another parameter to enable computation of gradients, `requires_grad=True`. Note that we don't want to do this for our data -- just our weights and biases. We set `requires_grad=False` for those instead. We'll also need to convert our input datum `x` to a float using the `float` method (to do math and stuff). Let's also make sure to do this for `y` to ensure that gradient calculation is disabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor(W, requires_grad=True)\n",
    "b = torch.tensor(b, requires_grad=True)\n",
    "x = torch.tensor(x, requires_grad=False).float()\n",
    "y = torch.tensor(y, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does our weight matrix look like now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about our bias vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that our input is also a `tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's examine our output label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we haven't yet computed gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do our linear transformation (matrix multiplication and addition):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.mm(W, x.double())\n",
    "Y = Y.double()\n",
    "Y = Y.add(b)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reshape our output from a column vector to a row vector. It'll become clear soon why we do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.shape)\n",
    "Y = Y.view(1, -1)\n",
    "print(Y.shape)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has a built-in `softmax` function. We import it from the `torch.nn.functional` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the output of our softmax function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = F.softmax(Y)\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we confirm that our `softmax` output sums to 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops! One more thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing cross-entropy loss in PyTorch, we just use the raw output -- it's not necessary to pass the output through a softmax function first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = F.cross_entropy(Y.view(1, -1), y)\n",
    "E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is easy in PyTorch! Just use `.backward()`. We can do this to our linear transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check out our gradients for our weight matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the bias vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't this magical? But wait, there's more. We don't even need to define our weights and biases manually. PyTorch gives us (at least) two options: the `torch.nn` module and its `torch.nn.Functional` API. We've already imported `nn.Functional` and used it for computing softmax and cross-entropy loss. Let's now import `torch.nn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `nn.Linear` function to define our weight matrix and bias vector. We just need to pass our number of input nodes (`in_features`) and output nodes (`out_features`), and specify `bias=True` to use biases. This is called a **fully-connected layer** since all of the input nodes are connected to all of the output nodes. Later, we'll encounter another architecture in which weights are shared and each input node is not connected to each output node. By convention, we call our fully-connected layer `fc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(25, 10)\n",
    "print(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view our weights with the `weight` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view our biases with `bias`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now let's construct our network the way it's supposed to be constructed. We define a class `Net` that inherits the properties of `nn.Module`, the \"base class for all neural network modules\" ([line 32](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py)) When we build a neural network, we construct a subclass of this class. We start by defining the class and our `__init__` constructor, then add our `fc` layer as a method of `self`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(25, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define how our input will **forward propagate** through the network. By convention, we define another class method `forward` that takes our randomly-initialized weights and biases from self and an input vector (or matrix of concatenated input vectors, but we'll get there eventually!) and produces our network output (from which we can then apply cross-entropy loss and update our weights and biases). We just need to pass our input through our fc layer and return the output (we'll add other stuff later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(25, 10)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view our network architecture by calling `net` as an instance of `Net()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much to see here! Don't worry, we're just getting started. Think of `Net` as an **architecture** that generates randomly-initialized untrained models (here, `net`). We then use instances such as net to train neural networks on data. We can now pass our input `x` through the `net` instance of `Net()` to generate a predicted model output `Y`. \n",
    "\n",
    "However, we first need to do two things to our input tensor `x`: change it from a (25, 1) column vector to a (1, 25) row vector using `view(1, -1)`, and cast it from a double to a float using `.float()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view(1, -1)\n",
    "x = x.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're good to go! What's our model output look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = net(x)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply cross-entropy loss to our model output `Y` and label `y`. Let's set the output to the variable `E` and call it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = F.cross_entropy(Y, y)\n",
    "E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's update our weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can loop over the **parameters** method of our **net** instance and print **param.data** to view our updated weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in net.parameters(): print(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, we've done a lot already, and we haven't even passed more than TWO points of data through our (absurdly simple) network. However, before we move on to lots of observations with real image data(!!!!) we're going to introduce a few more operations that we'll perform on our data to obtain more accurate classification results with greater computational efficiency.\n",
    "\n",
    "Let's think about how we're approaching the problem of image classification. Right now we're assuming that the way to go is to treat each pixel as an input, and to learn each connection between pixel and output class. Since we're dealing with a ridiculously small \"image\" (25 1-bit pixels), we don't have to learn too many parameters: 10x25 weights + 10 biases = 250 + 10 = 260 parameters.\n",
    "\n",
    "Now if we're dealing with a \"standard\" RGB image, we have to ingest a ton more pixels: 256 x 256 x 3 = 196,608 8-bit pixels in our input layer, each taking a value between 0 and 255 (and this is a \"small\" datum compared to a video or a genome, among other data). Even if we just apply a single matrix multiplication and addition to transform our data from a vector of length 196,608 to a vector of length 10, we still need to learn 196,608 x 10 weights + 10 biases = 1,966,090 parameters. That's a lot of parameters for a single linear transformation! Especially since it's completely insufficient for image classification. No wonder no one used neural networks for so long...\n",
    "\n",
    "This brings us to our second issue: a \"vanilla\" matrix multiplication and addition isn't the best way to extract information from an image, whether it be for the purposes of classification, object detection (drawing bounding boxes around objects of interest in an image), or semantic segmentation (coloring the pixels of objects of interest in an image).\n",
    "\n",
    "It turns out we can solve both of these issues with an operation called a **convolution**. We'll use our sample 5x5 \"zero array\" of one-bit pixels to demonstrate how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_zero_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we want to learn a set of weights and biases that'll connect our input pixels with our output. The output is a one-hot encoded vector corresponding to the \"zero\" label. Recall that we already constructed this array in NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_zero_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned our weights and biases (respectively) from a randomly-initialized 10x25 matrix and 10x1 vector. We called our weight matrix `W`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`b` is our bias vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption here is that each class is best learned (in terms of accuracy and computational efficiency) by summing up weighted values associated with each pixel for that class. Is this really optimal? An alternative is to use a **convolutional neural network** that learns sets of shared weights called **filters**, **kernels**, or **receptive fields** (do we really need four names for the same thing?) These shared weights take a particular **shape** (typically square or more generally rectangular). Let's start with the example of the smallest possible (1x1) filter. First we need to decide how many filters to learn. We call this our output or **volume**. Let's stick with volume since output can refer to other things. What if we only learned one filter? Let's randomly initialize our filter. We'll initialize it as an array even though it's just a single value. We'll stick with `w` to denote our weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(1)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to **stride** the filter across our \"image.\" At each point, we take the dot product of our (1x1) filter and the corresponding array of pixels of the same size (also 1x1). Let's start with a stride of 1! Our single volume $V$ is the (25x1) output of a for loop over the pixel vector. \n",
    "\n",
    "I'd like everyone to try the next set of cells individually. If you're stuck on something, work throught it a bit more, and then ask one of the people next to you for help. If you're still stuck, raise your hand and I'll come over and help. \n",
    "\n",
    "First, define a function `conv_1x1_stride1` that takes an input pixel vector `X`, randomly initializes a 1x1 filter, strides the filter one pixel at a time across the image, and returns the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass our specific input vector `a_zero_vector` to the function and store the result as `V` (for **volume**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we change the function for a stride of 2? Call this function `conv_1x1_stride2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we apply this convolution to our vector of zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we generalize our function to take a variable stride `s`? Call this `conv_1x1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we pass our image through `conv_1x1` with a stride of 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice work! We can also learn multiple volumes by generating a two-dimensional weight matrix `W`. The first dimension is the kernel size (in this case just 1), and the second dimension is our number of output volumes `k`, which we loop over to generate multiple outputs. I stored the result as a list of arrays; don't know if that's kosher. Let's examine this implementation together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_1x1(X, s, k):\n",
    "    V = []\n",
    "    if s > 0:\n",
    "        W = np.random.randn(1,k)\n",
    "        [V.append(X[0:len(X):s]*W[0][i]) for i in range(k)]\n",
    "        return V\n",
    "    return \"you can't do that\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we apply this 1x1 convolution to our zero vector with a stride of 5 and four output volumes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_1x1(X=a_zero_vector, s=5, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How might we define a 2x2 convolution? We first need to initialize a filter with random values. Again, let's assign our output to the weight matrix `W` and call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(2,2)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the region over which we'll stride. Let's view the \"image\" as a 5x5 array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = a_zero_array\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the value of the first row and second column of this array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_zero_array[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution is first applied to indices (0,0),(0,1),(1,0),(1,1). We then stride horizontally by $s$ steps. Let's start with a stride of one $(s=1)$ and a volume of one. We'll add 1 to each $y$, yielding indices (0,1),(0,2),(1,1),(1,2).\n",
    "\n",
    "Note that $x$ is the vertical index (from the top) and $y$ is the horizontal index (also from the top). This is kinda weird from a Cartesian perspective but makes sense in terms of how we define NumPy arrays.\n",
    "\n",
    "What are the dimensions of our volume matrix? \n",
    "\n",
    "Our width, $W_{2}$ is $(W_{1}-F+2P)/S) + 1$\n",
    "\n",
    "Our height, $H_{2}$ is $(H_{1}-F+2P)/S) + 1$\n",
    "\n",
    "and our depth, $D_{2}$ is $K$\n",
    "\n",
    "where $W_{1}$ and $H_{1}$ are the respective width and height of the input \"image.\" In this particular case, $W_{1} = H_{1} = 5$\n",
    "\n",
    "$F$ is the width and height of our filter(s). Here we assume that our filter(s) are square. $S$ is our stride. For our example, $F=2$ and $S=1$\n",
    "\n",
    "$D$ is the number of filters in our output volume, $V$. This is explicitly defined as a parameter (we're not going to do this in our first example (where $D=1$) but we will need to account for this in our generic convolution function).\n",
    "\n",
    "$P$ is something called **zero padding**. This adds $P$ layers of zeros to the border of our input image. Padding is useful when we want to use all the information provided by the image but the image and kernel size don't neatly align. For instance, we might want to use a 3x3 kernel on our 5x5 image with a stride of 3, but we would be unable to compute the dot product on the fourth and fifth rows or columns without striding across the border of the image. However, with one layer of zero padding we can now compute the first dot product on a row and column of zeros, plus the entries corresponding to the first and second row and column. Our second dot product will now use the top row of padding in addition to the intersection of the top two rows and rightmost three columns. We then use the zero padding for the first column of our third dot product. The fourth dot product is computed on the bottom 3x3 of the image (without padding). We're not going to use padding in our first example, but we want to make sure to include it when we build our more general function.\n",
    "\n",
    "Source: http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "So for our volume matrix of size $(W_{2},H_{2})$ (an instance of a volume tensor of size $(W_{2},H_{2},K_{2})$),\n",
    "\n",
    "$W_{2} = (W_{1}-F+2P)/S) + 1 = (5 - 2 + 2(0))/1 + 1 = 3/1 + 1 = 4$\n",
    "\n",
    "$H_{2} = (H_{1}-F+2P)/S) + 1 = (5 - 2 + 2(0))/1 + 1 = 3/1 + 1 = 4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do our first convolution. We can do this by indexing the top 2x2 slice of X and multiplying by W. Interestingly, this produces the elementwise product of two matrices, whereas `numpy.dot` is equivalent to `numpy.matmul` for matrices. Weird, huh. What's the output of this operation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W*X[0:2,0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we sum up over each of the columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(W*X[0:2,0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do another `sum`, we get the desired output of our convolution. Sum of elementwise product, dot product, or both? I'm not sure. I got my PhD in political science. What's the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(W*X[0:2,0:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to stride this thing. What's our next slice of the \"image\"? We just add one to the \"second\" dimension of our slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:2,0+1:2+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll continue to do this until we stride horizontally across the entire array. Then we'll stride vertically until we do the same in both dimensions. We can start again with a blank output list $V$. As we stride, we append our \"dot product\" (or whatever it is) to $V$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V=[]\n",
    "V.append(sum(sum(W*X[0:2,0:2])))\n",
    "print(V)\n",
    "V.append(sum(sum(W*X[0:2,0+1:2+1])))\n",
    "print(V)\n",
    "V.append(sum(sum(W*X[0:2,0+1+1:2+1+1])))\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what's going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V=[]\n",
    "m=0\n",
    "n=2\n",
    "V.append(sum(sum(W*X[0:2,m:n])))\n",
    "print(V)\n",
    "m+=1\n",
    "n+=1\n",
    "V.append(sum(sum(W*X[0:2,m:n])))\n",
    "print(V)\n",
    "m+=1\n",
    "n+=1\n",
    "V.append(sum(sum(W*X[0:2,m:n])))\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn our first horizontal stride into a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V=[]\n",
    "m=0\n",
    "n=2\n",
    "for i in range(5):\n",
    "    V.append(sum(sum(W*X[0:2,m:n])))\n",
    "    m+=1\n",
    "    n+=1\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good stuff. Now we can stride vertically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V=[]\n",
    "V.append(sum(sum(W*X[1:3,0:2])))\n",
    "print(V)\n",
    "V.append(sum(sum(W*X[1:3,1:3])))\n",
    "print(V)\n",
    "V.append(sum(sum(W*X[1:3,2:4])))\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so on... We're doing the same operation over again, this time starting with 1:3 instead of 0:2. Another loop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V=[]\n",
    "h=0\n",
    "k=2\n",
    "m=0\n",
    "n=2\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        V.append(sum(sum(W*X[h:k,m:n])))\n",
    "        m+=1\n",
    "        n+=1\n",
    "    h+=1\n",
    "    k+=1\n",
    "    m=0\n",
    "    n=2\n",
    "V=np.asarray(V)\n",
    "V=V.reshape(5,5)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! How would we turn this into a function? Let's call it `conv_2x2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_2x2(X):\n",
    "    W=np.random.randn(2,2)\n",
    "    V=[]\n",
    "    h=0\n",
    "    k=2\n",
    "    m=0\n",
    "    n=2\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            V.append(sum(sum(W*X[h:k,m:n])))\n",
    "            m+=1\n",
    "            n+=1\n",
    "        h+=1\n",
    "        k+=1\n",
    "        m=0\n",
    "        n=2\n",
    "    V=np.asarray(V)\n",
    "    V=V.reshape(5,5)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we pass our \"image\" through the function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_2x2(X=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to change the stride? That's just how much we're updating m and n in the inner loop and h and k in the outer loop. (You can also see how we might generalize to different horizontal and vertical strides, but we're not going to worry about that; no one really does that I don't think.) We'll go with a stride of 3, since that works for our 5x5 image with 2x2 filters (even though we'll lose information about the middle of our image - oh no! But then, we shouldn't be using a one-layer neural network to classify a single 5x5 array of 1-bit pixels, so no need to hand out the speeding ticket when there's a murder weapon in the trunk).\n",
    "\n",
    "Note that our horizontal and vertical ranges are simply the formulas for computing the volume of a convolution layer (or area of a single filter applied to the input image). We assume $F=2$, since we're applying a 2x2 filter; and $P=0$, since we're not applying any padding. We already know that $W_{1} = W_{2} = 5$. Thus, $i$ and $j$ loop over range $(5-2)/3 + 1 = 3/3 + 1 = 1 + 1 = 2$. We'll also need to convert the output of our division from float to integer with `int` (can't loop over a float). Let's give it a go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_2x2(X,s):\n",
    "    W=np.random.randn(2,2)\n",
    "    f=int((5-2)/s)+1\n",
    "    V=[]\n",
    "    h=0\n",
    "    k=2\n",
    "    m=0\n",
    "    n=2\n",
    "    for i in range(f):\n",
    "        for j in range(f):\n",
    "            V.append(sum(sum(W*X[h:k,m:n])))\n",
    "            m+=s\n",
    "            n+=s\n",
    "        h+=s\n",
    "        k+=s\n",
    "        m=0\n",
    "        n=2\n",
    "    V=np.asarray(V)\n",
    "    V=V.reshape(f,f)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we apply a 2x2 convolution with a stride of 3 to our input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_2x2(X,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! How about we add padding? There's actually a built-in function called `np.pad` that'll add zeros (by default) to an array. We can do this with `mode='constant'` and `pad_width=1` and store the result to `X` to update our input array (image) with zero padding of 1. We can thus set `pad_width=p` for generic zero padding. How do we implement this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_2x2(X,s,p):\n",
    "    X=np.pad(X, mode='constant', pad_width=p)\n",
    "    W=np.random.randn(2,2)\n",
    "    f=int((5-2+2*p)/s)+1\n",
    "    V=[]\n",
    "    h=0\n",
    "    k=2\n",
    "    m=0\n",
    "    n=2\n",
    "    for i in range(f):\n",
    "        for j in range(f):\n",
    "            V.append(sum(sum(W*X[h:k,m:n])))\n",
    "            m+=s\n",
    "            n+=s\n",
    "        h+=s\n",
    "        k+=s\n",
    "        m=0\n",
    "        n=2\n",
    "    V=np.asarray(V)\n",
    "    V=V.reshape(f,f)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a 2x2 convolution with our input array $X$, stride of 1, and one \"layer\" of zero padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_2x2(X=X,s=1,p=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same thing we did with our 1x1 convolution: repeat this operation `K` times for a volume of `K` outputs. This means we'll need `K` filters ${W_{1}, ... , W_{K}}$. In this notation, we use big $K$ since little $k$ is being used to index our array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_1x1(X, s, K):\n",
    "    V = []\n",
    "    if s > 0:\n",
    "        w = np.random.randn(1,K)\n",
    "        [V.append(X[0:len(X):s]*w[0][i]) for i in range(k)]\n",
    "        return V\n",
    "    else: \n",
    "        return \"you can't do that\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we extend this to a 2x2 kernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_2x2(X,s,p,K):\n",
    "    X=np.pad(X, mode='constant', pad_width=p)\n",
    "    W=np.random.randn(K,2,2)\n",
    "    f=int((5-2+2*p)/s)+1\n",
    "    V=[]\n",
    "    h=0\n",
    "    k=2\n",
    "    m=0\n",
    "    n=2\n",
    "    for i in range(f):\n",
    "        for j in range(f):\n",
    "            [V.append(sum(sum(W[0]*X[h:k,m:n]))) for q in range(K)]\n",
    "            m+=s\n",
    "            n+=s\n",
    "        h+=s\n",
    "        k+=s\n",
    "        m=0\n",
    "        n=2\n",
    "    V=np.asarray(V)\n",
    "    V=V.reshape(K,f,f)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we pass four outputs to our 2x2 convolution with a stride of 1 and padding of 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_2x2(X,s=1,p=1,K=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step! We want to generalize to any square convolution. We'll use $H$ to denote the height (and width, but $W$ is already our weights matrix). Furthermore, we'll replace that hard-coded 5 so that we can convolve on images of other sizes. We won't even assume they're square images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_2d(X,H,s,p,K):\n",
    "    F=X.shape[1]\n",
    "    G=X.shape[0]\n",
    "    X=np.pad(X, mode='constant', pad_width=p)\n",
    "    W=np.random.randn(K,H,H)\n",
    "    f=int((F-H+2*p)/s)+1\n",
    "    g=int((G-H+2*p)/s)+1\n",
    "    V=[]\n",
    "    h=0\n",
    "    k=H\n",
    "    m=0\n",
    "    n=H\n",
    "    for i in range(g):\n",
    "        for j in range(f):\n",
    "            [V.append(sum(sum(W[0]*X[h:k,m:n]))) for q in range(K)]\n",
    "            m+=s\n",
    "            n+=s\n",
    "        h+=s\n",
    "        k+=s\n",
    "        m=0\n",
    "        n=H\n",
    "    V=np.asarray(V)\n",
    "    V=V.reshape(K,f,g)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm sure this code can be cleaner, but let's just see if it works on a random 6x8 image with 2-bit pixels. `np.random.randint` is our friend here. First we'll generate our image. We'll call it $X$ again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(0,4,48)\n",
    "X = X.reshape(6,8)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a 2x2 kernel with stride of 1 and padding of 1. Don't ask why we're adding padding with a stride of 1 lol. Actually, do ask: it means you get what's going on! We'll generate 4 output volumes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_2d(X=X,H=2,s=1,p=1,K=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! We got something that resembles a convolution. Now let's go back to our 5x5 1-bit image example. Suppose we were to classify our image by applying a 2x2 convolution with stride of 1 and zero padding (zero zero padding?) Remember padding doesn't make too much sense with a stride of 1. We'll always apply our convolution to every pixel with a stride of 1. Let's also learn eight sets of shared weights (that's our \"output\" or \"volume\"). How many parameters do we learn?\n",
    "\n",
    "Let's see. It's 2x2=4 parameters for each filter/kernel/weight matrix, and we're going to learn 8 of those. So 2x2x8=32 parameters. That's not that many compared to what we had before (10x25+10=250+10=260 parameters for a 10x25 weight matrix and a bias vector of length 10). We could optionally add a bias to our convolutional layer (I'll leave this as an exercise for the reader. It's not too hard!) On the other hand, we've done a bit more work to compute our output for each filter, since we've had to stride the kernel across the entire array of pixels. \n",
    "\n",
    "But we can't possibly be done. We still need to output a probability vector, or at the very least some vector of length 10. We could simply use a fully-connected layer, or linear transformation to do this: flatten the output of our convolutional layer into a vector of length 32, multiply a 10x32 matrix by the vector, and we'll have our output vector of length 10! Pass that through a softmax and we're good to go (still need to backprop, but that's just calculus).\n",
    "\n",
    "However, there's another thing that the geniuses behind deep learning came up with, and it's called **\"max pooling\"**. How this works is you just take the maximum value for a specified area (receptive field) of each volume produced by a particular dot product of kernel and input, and then stride over the volume. As you stride horizontally, you output the maximum value into the first row of a new **downsampled** matrix. Once you reach the end of the input **feature map** (the output volumes of our convolutional layer are input feature maps to our pooling layer), you then stride vertically. Just like you're doing a convolution! I think this will make a lot more sense with an example.\n",
    "\n",
    "Let's take our 5x5 \"zero array\" image and apply our 2x2 convolution with stride 1 and output 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output = conv_2d(X=a_zero_array,H=2,s=1,p=0,K=4)\n",
    "conv_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how our convolutional layer changed the dimensions of our input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a_zero_array.shape)\n",
    "print(conv_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, we barely downsampled our image! That makes sense: we're applying small (2x2) filters, and we're only striding one pixel at a time, so we're reusing each pixel multiple times. With larger filter sizes, typically used on \"real\" images with many more pixels, we tend to downsample our images a good deal. Even with 3x3 filters and a stride of 2 (a common operation), you can imagine how this would work. Actually, we can try this out on our 5x5 image. Let's output four filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output2 = conv_2d(X=a_zero_array,H=3,s=2,p=0,K=4)\n",
    "print(conv_output2)\n",
    "print(conv_output2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been listing our output first, which is kinda unconventional, but you get the idea what's going on here. We downsample the area of the input array from 5x5 to 2x2. At the same time, we increase the depth of our image from 1 to 4. In total, we go from 5x5=25 pixels to 2x2x4=16 weights.\n",
    "\n",
    "Now let's return to our 5x5 array. Here we went from 5x5=25 pixels to 4x4x4=64 weights! How do we downsample? Let's try pooling. Our first dimension is the output volume. We can extract a single volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our pooling operation. It's the same as our convolution: we have a (typically square) shape (let's say 2x2) and a stride (let's say 2). Let $C$ be the output of our convolutional layer, $H$ be the height and width of the pooling region, and $s$ be our stride.\n",
    "\n",
    "For a square pooling region accepting a volume with equal width and height $W_{2} = H_{2}$, the pooling width and height $W_{3} = H_{3} = (W_{2} - F)/s + 1$, where $F$ is the width and height of the pooling region and $s$ is the pooling stride. The pooling depth $D_{3}$ is equal to the volume depth $D_{2}$.\n",
    "\n",
    "NumPy has an operation `np.amax` that returns the maximum value of an ndarray. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_2x2(C,F,s):\n",
    "    H=int((C.shape[1]-F)/2+1)\n",
    "    P=[]\n",
    "    h=0\n",
    "    k=F\n",
    "    m=0\n",
    "    n=F\n",
    "    for q in range(C.shape[0]):\n",
    "        for i in range(H):\n",
    "            for j in range(H):\n",
    "                P.append(np.amax(C[q][h:k,m:n]))\n",
    "                m+=s\n",
    "                n+=s\n",
    "            h+=s\n",
    "            k+=s\n",
    "            m=0\n",
    "            n=2\n",
    "        h=0\n",
    "        k=F\n",
    "        m=0\n",
    "        n=F\n",
    "    P=np.asarray(P)\n",
    "    P=P.reshape(C.shape[0],F,F)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine the output of our pooling operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = pool_2x2(C=conv_output,F=2,s=2)\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've also downsampled further, cutting our width and height in half while retaining a depth of 4. Again, it's not conventional to list the depth first, but it prints nice and works out well with the list comprehension in the convolutional layer (okay, I could have just added a third for loop like I did with the pooling layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(P.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to get from our pooling layer to our output layer, and finally to our softmax layer. Really at any time we can just flatten our output into a vector of length $m$ and multiply an $nxm$ matrix by this vector to get a vector of length $n$. That's essentially what we'll do, but we still have one more operation to add. We'll get to it soon.\n",
    "\n",
    "All we've covered so far should also make us appreciate how easy it is to construct convolutional and pooling layers in PyTorch. Recall our old neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(25, 10)\n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now replace our fully-connected (or **dense**) layer for our 2x2 convolutional layer with stride of 1 and volume of 4! PyTorch's nn module makes this super easy with `nn.conv2d`. `2d` refers to the dimensions of the kernel. We don't even have to define the kernel size and stride (though we can). We simply pass our number of input channels `in_channels` (we only have 1 channel for our single image of 1-bit pixels), `out_channels` (our volume of 4), `kernel_size` (2, PyTorch assumes square kernels by default) and `stride` (2, with horizontal and vertical stride equal by default). By convention, we call our first convolutional layer `conv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=1,out_channels=4,kernel_size=2,stride=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can just pass the values of the arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=1,out_channels=4,kernel_size=2,stride=2)\n",
    "print(conv)\n",
    "conv2 = nn.Conv2d(1,4,2,2)\n",
    "print(conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine our weights with `conv.weight`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also adds a bias by default. You can specify `bias=false` if you don't want this, but why not add a bias? Biases are computationally cheap (if that matters when doing matrix math on a 5x5 image of one-bit pixels). We can examine them using `conv.bias`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can swap in our `conv` for `fc` in our `init` and `forward` modules. Let's do this together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining a `net` instance of our `Net()`, we see that we have one two-dimensional convolution layer with a batch size of 1, output volume of 4, and 5x5 kernels with a stride of 2 in both directions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add our pooling layer `nn.MaxPool2d`. We pass (2,2) for the size and stride of our receptive field. By convention, we'll call this `pool`. How do we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out our new **multilayer** (deep??) neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like `MaxPool2d` has a few other default parameters. You might recognize `padding` from our convolution operation. We can also pad our input with zeros for pooling. For `padding=N`, `N` is just the size of the padding (how many zero cells extending horizontally or vertically from the original border of the image).\n",
    "\n",
    "`dilation` refers to spacing between values in a convolutional or padding kernel. For instance, if we applied our 2x2 kernel with a `dilation rate` of 2, we would end up applying our operation over a 3x3 field but only performing elementwise multiplication on every other row or column.\n",
    "\n",
    "A visualization DEFINITELY helps to explain dilation (and other convolutional operations!) https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\n",
    "\n",
    "What about `ceil_mode`? Remember when we computed the dimensions of the output of our convolutional layer? We can do the same for the output of our pooling layer. We don't need to go over the details of the operation (see https://pytorch.org/docs/stable/\\_modules/torch/nn/modules/pooling.html if you're curious). The point is that our dimensions must be (positive) integers, and thus if the operation produces a non-integer output, we'll either need to round down (floor) or up (ceiling). When `ceil_mode=False` (by default), PyTorch uses the floor of the operation rather than the ceiling to compute the dimensions of the pooling output.\n",
    "\n",
    "Before we add our fully connected and softmax layers, we're going to introduce one final operation. When we \"do\" \"**deep learning\"** (deep learn?), we're stacking multiple layers of (frequently) linear operations. But we also want to introduce nonlinear operations so that our neural network \"function\" can better approximate nonlinear relationships between our inputs and outputs. There are other good reasons for introducing nonlinearities, but let's save the theory for another time.\n",
    "\n",
    "The particular class of nonlinear operations we'll examine here are called **activation functions**. If we think back to our conceptualization of a neural network as a set of nodes organized into layers and connected via (fully connected, convolutional, and other) operations, we're (crudely) modeling a neurological process whereby input data (axons) are transmitted into outputs. But what tells us whether (or how) our neuron will \"fire\" or \"activate\"? Here's where activation functions come in.\n",
    "\n",
    "Technically, activations don't need to be nonlinear. Let's define a random input to our activation function (a vector of length 8):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(8)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now define a simple (hypothetical, pointless) activation function `identity` that returns its input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(X):\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be pretty obvious what's going to happen if we pass $X$ to the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to go a \"step\" further and return 0 if $x_{i}$ is below a certain threshold (say, 0) and 1 if $x_{i}$ is at or above the threshold, for each $x_{i}$ in vector $X$. For fun, you could generalize this to a generic tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(X):\n",
    "    for i in range(len(X)):\n",
    "        if X[i] < 0:\n",
    "            X[i] = 0\n",
    "        else:\n",
    "            X[i] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the output of this function for a random vector `X` (length 8). Let's print `X` first for comparison, then apply `step`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(8)\n",
    "print(X)\n",
    "step(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we've got a few issues with our step function. It's not smooth, it's coarse, and most problematically it's derivative is zero at all points. That means when we backprop we're going to kill all the gradients. No good.\n",
    "\n",
    "But what if we combine these two completely useless activation functions: the one (identity function) that doesn't update outputs during the forward pass with the one (step function) that doesn't update weights during the backward pass? Now we have the (no joke) go-to activation function for convolutional neural networks: the **rectified linear unit (ReLU)**, or less pretentiously, the \"thing that sets our negative values to zero.\" As we did with the implementation of convolutions, I'd like to have everyone try to build the next few blocks of code individually. If you get stuck, work through it some more. If you're still stuck, ask someone next to you for help. After that, ask me!\n",
    "\n",
    "First, code the `ReLU` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create another random vector `X` of length 8, print it, and apply your `ReLU` function to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now add this to our network architecture. Of course PyTorch's functional API (which we imported as `F`) has a built-in function for ReLU, `F.ReLU`. Add it to our `Net()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we add our softmax and loss function and define our backpropagation, we still need a fully-connected layer to transform the output of `self.pool(F.relu(self.conv(x)))` to a vector of length 10 (again, the number of classes).\n",
    "\n",
    "Let's review how the dimensions of our image have changed as the image is passed through the network. We began with a 5x5 image of 1-bit pixels and converted it to a PyTorch tensor `X`. We called the `float()` method on the model to convert the model to a float. Let's do this again, since our `X` is probably something else by now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(a_zero_array, requires_grad=False).float()\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we apply four 2x2 convolutions with stride of 1. Before we pass `X` to `nn.Conv2d`, we need to convert the tensor to size \\[batch_size, channels, height, width\\]. Right now it's of size \\[5,5\\]. `view` does the trick, adding dimensions for batch_size (just our 1 image) and input_channels (the one \"color channel\" of 1-bit pixels). Let's apply the `view` method to `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.view(1, 1, 5, 5)\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply `nn.Conv2d` to `X`. We'll go with the 1 input channel, 4 output channels, a kernel size of 2, and a stride of 2. Let's set the output to `C`, print its shape, and call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(1,4,2,2)\n",
    "C = conv(X)\n",
    "print(C.shape)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does our pooling operation change the shape? What happens when we apply two-dimensional max pooling with a stride of 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = nn.MaxPool2d(2,2)\n",
    "P = pool(C)\n",
    "print(P.shape)\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see now that we've downsampled our input so much that we actually have **fewer** nodes than classes! We need a linear transformation that'll get us from four nodes to 10. Sounds like a job for a 10x4 matrix, with a(n optional) 10x1 vector!\n",
    "\n",
    "Before we do this, we need to **flatten** the output of our pooling operation using `view`. We can hard code this as `P.view(4)`, since the number of total features should be clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = P.view(4)\n",
    "P.shape\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember in PyTorch, we don't need to specify the dimensions of a weights matrix and bias vector. We can just call `nn.linear` with `use_bias=True` and specify our number of input nodes (4) and output nodes (10). We apply this to the reshaped output of the the convolution, relu, and pooling operation. Since we're already passing the output of our fully-connected (**dense**) layer through a softmax function, we don't need an additional ReLU activation. What does our updated network look like? Let's build it together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, printing our `net` instance of `Net()` only displays the layers defined in our `__init__` constructor. Let's do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we don't need to apply our cross-entropy loss to a softmax transformation of our output. `F.cross_entropy` will take care of the softmax for you. Since we're working in `nn` (and I'm using this as a reference: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html), we'll use the `nn` implementation of cross-entropy: `nn.CrossEntropyLoss()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need an **optimizer**. It turns out that \"vanilla\" gradient descent (where we subtract the scaled partial derivatives of our loss function with respect to each of our weights and biases) for each of our training samples isn't usually implemented in practice (horror face emoji). Instead, we use something called **stochastic gradient descent (SGD)**. Unlike \"vanilla\" or **batch gradient descent**, which calculates the gradient using the entire training dataset before updating weights, stochastic gradient descent updates the gradient using data from one observation at a time. Each observation is randomly selected from the dataset, hence \"stochastic.\"\n",
    "\n",
    "Of course, none of this matters when we're feeding a single observation through the network. But we'll use SGD because it tends to work better than batch gradient descent for large datasets. Hey if we were selecting an optimal algorithm for classifying a single 5x5 image of one-bit pixels, we probably wouldn't use a neural network. (We'd probably just give up...)\n",
    "\n",
    "PyTorch has an optimizer package called `torch.optim`. If you really want to nerd out on this stuff, there are a ton of optimizers, and many of them are in this package. If you know one that isn't, you could contribute it to PyTorch! A lot of these optimizers add some form of momentum to prevent the network from getting stuck in a local minimum. We won't worry about that for now. Let's import our optimizer package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our optimzer using `optim.SGD`. First we'll need to pass our `net.parameters()` to the optimizer. This is just our weights and biases (prior to updating):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll add our learning rate, `lr`. We can stick with `lr=0.1`. We'll leave out **momentum** for now, but you can experiment with adding this argument.\n",
    "\n",
    "PyTorch classifier tutorial: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\n",
    "\n",
    "torch.optim.sgd source code: https://pytorch.org/docs/stable/\\_modules/torch/optim/sgd.html\n",
    "\n",
    "An incredible (and comprehensive) summary of optimization algorithms: http://ruder.io/optimizing-gradient-descent/\n",
    "\n",
    "Let's define our optimizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we feed our image into our network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we apply cross-entropy loss on our \"raw\" output (not passed through the softmax function). However, we can still apply `F.softmax` to our raw output to view our probability vector (tensor). What do we get here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = F.softmax(output)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the index will be equivalent to the value of the digit, so we can just apply `torch.argmax` (which gives us the index of our maximum value) to our \"probability tensor\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the network predicts a \"5.\" Remember, our ground truth is a \"0.\" This isn't a big deal. We haven't updated our weights and biases yet, so this is entirely random. Before we update our parameters (using this one example!), let's compute our loss (on our `output`, prior to applying softmax):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ce_loss(output, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that this is the negative natural logarithm of the predicted output for the zero index. As with other NumPy operations, we can just swap `torch.log` for `np.log`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-torch.log(predictions[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute our gradients. We pass the argument `retain_graph=True` so that we can reuse this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update parameters by passing the `step()` method to our optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do another forward pass of our \"data\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our loss has decreased:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ce_loss(output, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe it's even getting closer to the prediction...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course in the \"real world\" we train neural networks with many more parameters, on observations with many more features, on datasets with many more observations. \n",
    "\n",
    "Hopefully what our example has done is to enable you to learn the **mechanics** of training a neural network. With a single 5x5 array (or tensor) of binary pixels, it is easier to wrap one's head around the process of learning weights and biases through matrix multiplications and additions, convolutions, activations, and softmax functions.\n",
    "\n",
    "Nevertheless, there's a lot that we haven't covered. We didn't really go over the practical issues involved in training a neural network with real data: stuff like data ingestion, resizing and augmenting data, partitioning our data into training and test sets, and even simple things like writing print statements to record our loss or outputting our trained model files at different checkpoints.\n",
    "\n",
    "Let's recap what we've learned!\n",
    "\n",
    "We began by exploring the simple case of linear regression, first with a single weight, and then with a weight and a bias. While we can use a neural network to learn these parameters, it usually makes more sense to solve for our parameters analytically.\n",
    "\n",
    "From linear regression, we moved to using a **fully-connected** network (matrix multiplication and addition) to transform our input into output. We examined the logic of **forward propagation** (computing output and loss) and **backward propagation** (or **backpropagation**: updating weights and biases) using NumPy.\n",
    "\n",
    "Although NumPy operations are great for learning the logic of training a neural network, we can save a lot of time by using PyTorch. Let's conclude by reviewing the simple network we constructed. \n",
    "\n",
    "We define a `Net` class comprised of a **network architecture** (`__init__`) and our forward propagation, or **forward pass** (`forward`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv2d(1,4,2,2)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc = nn.Linear(4,10)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv(x)))\n",
    "        x = x.view(-1, 4)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our architecture consists of three **layers**:\n",
    "\n",
    "1) A **convolutional layer**, which slides a matrix (filter) of shared weights across the input, computes the dot product at each point, and repeats the operation for an arbitrary number of filters.\n",
    "\n",
    "2) A **pooling layer**, which computes the maximum value of each input over a specified region.\n",
    "\n",
    "3) A **fully-connected (dense) layer**, which performs a linear transformation (matrix multiplication and addition) on the input.\n",
    "\n",
    "We then define three operations in our **forward** pass:\n",
    "\n",
    "1) A \"conv-relu-pool\" triple whammy, whereby we pass `x` through the convolutional layer, apply our `ReLU` activation, and max pool the output.\n",
    "\n",
    "2) A \"flatten\" operation, `x.view`, whereby we reshape our output into a one-dimensional tensor (vector?) or length 4.\n",
    "\n",
    "3) A \"linear\" operation, whereby we \"upsample\" from our flattened output to a \"class vector\" of length 10.\n",
    "\n",
    "Now we can compute our output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "print(net)\n",
    "output = net(X)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...display our predicted probabilities and predicted class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = F.softmax(output)\n",
    "print(predictions)\n",
    "prediction = torch.argmax(predictions)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...calculate our loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "loss = ce_loss(output, y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..and backpropagate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've covered a lot for an afternoon. Hopefully you now understand a bit more about what's going on when you train a neural network. Once you get the logic behind the training process, the rest is just engineering...\n",
    "\n",
    "Please send feedback to pdonnelly@groupwaretech.com!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
